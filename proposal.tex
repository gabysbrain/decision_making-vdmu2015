Visualization research right now is divided into two distinct fronts. On
one side are lab studies while on the other side are design studies.

Lab studies concentrate mostly on perceptual aspects which are low-level visual
understanding and are considered to be more or less fixed across humans
(barring exceptions like color-blindness). Furthermore, the testing
methodologies for studies such as these are relatively mature with fixed
objectives and protocols. However, what they can measure is limited but highly
generalizable. Most studies done this way deal with preattentive processing or
very simple tasks. Because of this, visualizations tested in this way must be
boiled down to their raw elements and even adding simple interactions can make
getting useful results difficult.  Furthermore, it is not always clear exactly
how to apply the results of these studies in when designing visualization
tools. There is much more than perception to building a tool that can be used
in a real-world context though.

On the other hand, design studies are highly focused on a particular
application scenario. While not as generalizable, the end goal of a design
study is a tool that is designed to be used in a real-world application
scenario. In addition to selecting proper visual encodings that are
perceptually accurate, the integration of multiple views as well as
interactions between these views must be taken into account. In the course of
designing one of these tools one must also carefully consider what tasks the
intended users want to accomplish as well as how they will employ these tasks
to complete their analysis goal. However, the feedback in design studies is
necessarily qualitative and fuzzy. A new visualization tool involves changing
many, many variables and any effect seen as a result of a focused lab study
(for example) could be attributed to any combination of these changes.
Therefore, while some of the knowledge gained in designing these tools is
\emph{transferable} to other visualization tools, the results are often not
\emph{generalizable} to the larger populace. Part of this is because different
users have different problem solving strategies. (point out shortcomings of
Gestalt theory?)

There is still a large gap in research between the results from user studies
and design studies. Some researchers have attempted to bridge this gap with
task taxonomies (cite Bremer and another one) or models (cite Keim and
another). These are largely focused on ``what'' users are doing though and not
``why'' they are doing it (even Bremer). What is missing is a better modeling
of the user from psychological principles, namely, what is the overall problem
solving process of the user in terms of solving the problem of extracting
information from data? By making user modeling more explicit we could shorten
the number of iterations for design studies since we would have a better
understanding beforehand what visual encodings and interactions will help the
user accomplish their goal beyond perceptual understanding. In addition, better
user modeling will reduce the number of ``unknown independent variables'' when
conducting a user study and will lead to being able to run lab studies on much
more complex visual encodings and interactions and reason about the results
better (and is another independent variable).

For example, we ran a lab study with two different interfaces in order
to test if a simple visual encoding of the sensitivity of a parameter to
a simulation model will make a more effective visualization. One issue
with this system is the relatively open-ended nature of the task: the
user could select any reasonable value for the expected return and still
complete the study. Before looking at the results we found that the
participants in the study could be divided into two different groups
based on their strategy in selecting their portfolio. Based on the
strategy we we able to find that the ``sweet spotters'' had a
statistically significant preference for the SA interface over the
other, while the ``risk fixer'' group showed no clear interface
preference. We did not anticipate the two different groups when
designing the study (we thought everyone would be a ``sweet spotter'')
but knowing that there would be a way to classify the users beforehand
would have beeen helpful. With better user characterization this would
have been possible.

The visualization community has already started examining personality types and
how those correspond to visualizations. While this does make the visualization
choice more ``personal'', psychological personality types are designed to be
consistent over time and therefore cannot account for the case where the same
user will use different problem solving strategies depending on the
task~\citep{ref}. We argue for a more situation-based approach to modelling a
user interacting with a visualization tool. We propose to look at the entire
visual exploration process as a case of solving a complex problem. Complex
problem solving can be defined as: ``CPS occurs to overcome barriers between a
given state and a desired goal state by means of behavioral and/or cognitive,
multi-step activities''~\citep{Frensch:2005}. Within this framework is insight
which has been defined in many ways but all definitions are remarkably similar
to the definition from Mayer for learning which is ``information aquisition''
and ``knowledge construction''~\citep{ref}. Even with these definitions though
there is still the question of how to measure insights in a visualization
system. We feel that the methods from e-learning research can be used to
measure the aquisition of insights.

Similarly, one of the major reasons for using a visualization system,
especialy one with a model behind it, in the first place is for making
actionable decisions. To this end, we should look at the work that
researchers have done on decision making where they have various
heuristics how people go about making decisions. Here, decisions are
defined as making a selection for a ``best'' candidate from a set of
possible options (need to double check the exact definition).

In this proposal we begin the overall examination of this process and
its relevance and benefits to visualization research by first looking
into the decision making heuristics but later we plan to examine the
entire model. This is a huge project though.

In short, we want to examine how decision making relates to the usefulness in
visualization and implications for design of visualization tools.

And, as a result of this proposal we would like to introduce the following
questions for discussion: is visualization already doing this, what types of
users are there, the common decision making strategies in visualization
systems, and should we be steering users to certain decision making strategies.

\section{Heuristics}\label{heuristics}

All these heuristics assume that the decision maker (or user in this case)
is selecting a decision item from a list of discrete choices. Each item
has multiple attributes (objectives). These objectives are competing with
each other in the sense that there is no clear optimal choice. What is also
important about the heuristics is that depending on how information is 
presented to participants different decisions heuristics will be employed
and different optimal selections will be made~\citep{ref}.

\begin{table*}[tb]
  \begin{center}
    \caption{Decision types}
    \inputfile{paper_codes}
  \end{center}
\end{table*}

To see if users indeed use different strategies we coded the 21 papers
from the visual parameter space analysis paper \citep{Sedlmair:2014}
with the decision heuristic types above (or none). We found that 6/8 of
the papers described some sort of heuristic when describing the task
analysis, user characterization, or case study.

%\subsection{Experts in expert %situations}
%\label{experts-in-expert-situations}

\subsection{Weighted additive rule}\label{weighted-additive-rule}

The weighted additive rule, proposed by \ttwnote{who?}, considers all
attributes of the various choices at once and directly considers 
the weights. The user selects weights for the various attributes and 
the option with the heighest weighted sum of attributes.
A version of this strategy considers all attributes evenly but still
the selection is made based on the total weighted value.

Practical issues with this include mapping attributes to numerical values
and examining the effects of adjusting the weights. In our evaluation of
design studies this rule is not directly supported. We considered that
systems supporting the evaluation of distance functions such as 
in systems such as ParaGlide~\citep{Bergner:2013} as using this strategy.
LineUp~\citep{Gratzl:2013} is the only system as far as we know that
directly supports the evaluation of numerical weights on objectives.

\subsection{Lexicographic}\label{lexicographic}

Rather than considering all attributes at once the user can instead simply
order the selections by the most important attribute to them. Ties within this
attribute are broken by sorting by the second most important attribute and so
on until a clear candidate appears at the top. This process as been named
lexicographic~\citep{ref} by the decision making community.

In this visualization literature, this is a realatively common strategy.
Many visualization systems, for example, Tuner~\cite{Torsney-Weir:2011} and
Luboschick et al.~\citep{Luboschik:2014} allow the user to prioritize a few
objective measures and evaluate the outputs based on those few. Naturally,
when presented with more attributes than can be easily compared between
one of the standard visualization responses to this issue is to filter
these attributes down to just the core set. This is a form of dimension 
reduction.

\subsection{Elimination by aspects}\label{elimination-by-aspects}

Rather than filtering in order to find the optimal decision, one could
also set thresholds and filter out the unacceptable decisions. This process
is repeated until a final choice remains. This process is called
the majority of confirming dimensions~\cite{ref} strategy. 

This heuristic is frequently employed in many visualization systems with
perhaps the system developed by Spence et al.~\citep{Spence:1995} being a
direct application of this heuristic. The user can interactively filter
objective (output) dimensions into acceptable and unacceptable regions and
see that effect on the selections in the parameter space. 
Vismon~\cite{ref} also uses a similar strategy.

\subsection{Frequency of good/bad
features}\label{frequency-of-goodbad-features}

If, like in the elimination by aspects strategy, one can articulate for
each attribute a good and bad level then one could label each attribute
of each decision option by this. Then one could simply count the number of
attributes that are labeled "good" and compare this against the number of
"bad" labels and select the option with the highest difference. This has
been called the frequency of good and bad features~\citep{ref}.

This strategy is never directly addressed by the visualization community
but the closest example we could find was the work by 
Coffey et al.~\citep{Coffey:2013}. In their system the user could browse
through simulations and find similar ones which may be closer to what
the user is looking for \ttwnote{clean up}.

\subsection{Satisficing}\label{satisficing}

Satisficing~\cite{ref} does not consider the entire set of decision options
holistically. Rather, when employing this strategy, one considers each 
option one at a time. Whether all attributes of the choice are considered or
just a few depends on the person but either way the first acceptable option
encountered is selected.

We did not see any evidence in the visualization of this strategy being
employed. In some ways it is at odds with visualization tool development.
Usually, one assumes that all data must be examined. Selecting the number
of samples to take from a simulation, however, may suffer from this since
the choice of if sufficient samples have been taken is never evaluated
in systems such as Tuner~\citep{Torsney-Weir:2011} or 
Paramorama~\citep{Pretorious:2011}. \ttwnote{maybe delete this...}

\subsection{Majority of confirming
dimensions}\label{majority-of-confirming-dimensions}

As an alternative to looking at all decision options at once one could 
instead examine them in a pairwise manner. The "winner" of each pairing is
compared against the next choice and so on until a final optimal choice
prevails.

This is also not very common in visualization perhaps because the pairwise
comparison is quite labor intensive. Also, like satisficing, depending on the
order that the options are considered one may get a very different outcome. In
Fluid Explorer~\cite{Bruckner:2010} and Paramorama~\citep{Pretorious:2011} this
is done by the user manually selecting an optimal candidate simulation from 
a list of simulation outputs.
\ttwnote{Eric Brochu's work is probably the best example}


\section{Questions}\label{questions}

During the workshop we would like to address a number of questions.

\subsection{Is vis already doing this?}
\label{is-vis-already-doing-this}

One question is do these problem solving heuristics exist already in the
visualization literature but just in a different name? From the literature
search we conducted the answer is no. There are elements of task support for
these heuristics, like filtering, but a comprehensive strategy is never
identified. Visualization systems are often designed around 
"overview first, zoom and filter, details on demand"~\cite{Shneiderman} or
a "global to local" search strategy~\cite{Sedlmair:2014}. These may help to
address the more global heuristics like weighted additive or lexicographic.
However, this is at odds with the more comparative heuristics like majority
of confirming dimensions. \ttwnote{more?}

By addressing these heuristics more directly we could \ttwwarning{What?}
So far, this does seem quite useful. For example, the local to global
strategy, identified in Sedlmair et al.~\citep{Sedlmair:2014} could very
easily suffer from the limitations of satisficing.


\subsection{What types of users are there?}
\label{what-types-of-users-are-there}

Lexicographic and elimination by aspects are the most popular heuristics
by far. Both of these concentrate on one parameter at a time and are
about filtering alternatives.

\subsection{Should we be steering users to certain decision-making strategies?}

We see this as an open question. Visualization concentrates on information
presentation. There is evidence that the presentation of information
will influence which decision making strategy one employs~\citep{ref}.
On the other hand, in real-world decision making there is often no clear
optimal decision to be made so it is difficult to evaluate which decision
making heuristic is best. However, it seems that it is best to at least
comprehensively examine all the options.

\subsection{Common decisions in vis systems}
\label{common-decisions-in-vis-systems}

Based on our literature survey we found certaint areas of parameter space
exploration where a decision is being made.  For example, is the sampling
sufficient, what model to select, and what parameter settings are good?

\subsection{Weighted parameter lists are
missing}\label{weighted-parameter-lists-are-missing}

Only one of the papers, ParaGlide~\citep{Bergner:2013}, concentrated on a
weighted strategy. Furthermore, ParaGlide did this though computing and
investigating a distance metric rather than directly adjusting the
weights. We're not sure why this is not a very common strategy in the
tools we examined. As far as we know, there is only one tool published,
LineUp~\citep{Gratzl:2013}, that allows the user to directly see the
effect of manipulating weights.

It's not very clear why this is not so common perhaps just momentum? Is
this hidden in other visualization systems under the monkier of
``distance metric''? Also, weighting objectives is a relatively
numerical process and perhaps this is simply done without visualization
support where, for example, numerical optimization would be very
effective?

\section{Conclusion}

We have proposed a task-centered idea of evaluating the decisions users
make while evaluating uncertainty. We have enumerated some common decision
making heuristics and evaluated if these are currently addressed by 
visualization systems today. We found that these heuristics are not directly
addressed by current systems and offered up several questions for
discussion going forward.

